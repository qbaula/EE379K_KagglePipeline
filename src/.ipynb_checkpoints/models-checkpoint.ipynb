{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, ExtraTreesClassifier, BaggingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, train_test_split, cross_val_score, cross_val_predict\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load clean training and test data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data (dataset_type):\n",
    "    train = pd.read_csv('../data/processed/train_' + dataset_type + '.csv', index_col=0)\n",
    "    test = pd.read_csv('../data/processed/test_' + dataset_type + '.csv', index_col=0)\n",
    "\n",
    "    y_train = train['Y']\n",
    "    X_train = train.drop (['Y'], axis=1)\n",
    "    \n",
    "    return (train, test, X_train, y_train)\n",
    "\n",
    "def check_missing_values (df):\n",
    "    if df.isnull().any().any() == False:\n",
    "        print ('No missing values')\n",
    "    else:\n",
    "        print ('Contains missing values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49998, 28)\n",
      "(50000, 27)\n",
      "No missing values\n"
     ]
    }
   ],
   "source": [
    "train_basic, test_basic, X_train_basic, y_train_basic = load_data('fill')\n",
    "print (train_basic.shape)\n",
    "print (test_basic.shape)\n",
    "check_missing_values (train_basic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49998, 28)\n",
      "(50000, 27)\n",
      "Contains missing values\n"
     ]
    }
   ],
   "source": [
    "train_missing, test_missing, X_train_missing, y_train_missing = load_data('missing')\n",
    "print (train_missing.shape)\n",
    "print (test_missing.shape)\n",
    "check_missing_values (train_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49998, 34)\n",
      "(50000, 33)\n",
      "No missing values\n"
     ]
    }
   ],
   "source": [
    "train_inter, test_inter, X_train_inter, y_train_inter = load_data('inter')\n",
    "print (train_inter.shape)\n",
    "print (test_inter.shape)\n",
    "check_missing_values (train_inter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49998, 28)\n",
      "(50000, 27)\n",
      "No missing values\n"
     ]
    }
   ],
   "source": [
    "train_scaled, test_scaled, X_train_scaled, y_train_scaled = load_data('scaled')\n",
    "print (train_scaled.shape)\n",
    "print (test_scaled.shape)\n",
    "check_missing_values (train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49998, 28)\n",
      "(50000, 27)\n",
      "No missing values\n"
     ]
    }
   ],
   "source": [
    "train_log, test_log, X_train_log, y_train_log = load_data('log')\n",
    "print (train_log.shape)\n",
    "print (test_log.shape)\n",
    "check_missing_values (train_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train_basic.to_csv('../ensemble/train/y.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_folds(X, y, splits):\n",
    "    kf = StratifiedKFold(n_splits=splits, shuffle=True, random_state=24601)\n",
    "    folds = list(kf.split(X,y))\n",
    "    \n",
    "    return folds\n",
    "    \n",
    "def train_and_save_preds(model, folds, X_train, y_train, test, csv_name):\n",
    "    # Train model on the folds defined\n",
    "    result = train(model, folds, X_train, y_train, test)\n",
    "        \n",
    "    # Output train predictions to a csv  \n",
    "    train_preds_csv = pd.DataFrame(columns=['Y'], index=X_train.index, data=result['train_preds'])\n",
    "    train_preds_csv.to_csv('../ensemble/train/' + csv_name + '_train.csv')\n",
    "    \n",
    "    # Output test predictions to a csv\n",
    "    test_preds_csv = pd.DataFrame(columns=['Y'], index=test.index, data=result['test_preds'])\n",
    "    test_preds_csv.to_csv('../ensemble/test/' + csv_name + '_test.csv')\n",
    "\n",
    "    print ()\n",
    "    sanity_check (y_train, test_preds_csv)\n",
    "    sanity_check (y_train, train_preds_csv)\n",
    "    \n",
    "    return result['model']\n",
    "\n",
    "def train (model, folds, X, y, T):\n",
    "    \"\"\" Train the model using stratified cross validation.\n",
    "    \n",
    "    Say there are K folds, train the model on (K-1) folds \n",
    "    and predict both on the test set and the holdout fold.\n",
    "    Repeat so that all K folds have been held out once.\n",
    "    \n",
    "    Average all K test predictions to create the final predictions.\n",
    "    Then, combine all the predictions on the held out fold to \n",
    "    create the predictions on the training set. The training set \n",
    "    predictions will be used for the second level training.\n",
    "    \"\"\"\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    T = np.array(T)\n",
    "        \n",
    "    train_preds = np.zeros(X.shape[0])\n",
    "    test_preds = np.zeros(T.shape[0])\n",
    "    test_preds_i = np.zeros((T.shape[0], len(folds)))\n",
    "    \n",
    "    for i, (train_idx, test_idx) in enumerate(folds):\n",
    "        X_train = X[train_idx]\n",
    "        y_train = y[train_idx]\n",
    "        X_holdout = X[test_idx]\n",
    "        \n",
    "        %time model.fit(X_train, y_train)\n",
    "        \n",
    "        train_preds[test_idx] = model.predict_proba(X_holdout)[:,1]\n",
    "        test_preds_i[:, i] = model.predict_proba(T)[:,1]\n",
    "        \n",
    "    test_preds[:] = test_preds_i.mean(1)\n",
    "        \n",
    "    return {'model':model, 'train_preds':train_preds ,'test_preds':test_preds}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "__folds = generate_folds (X_train_basic, y_train_basic, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_val (model, X_train, y_train, n_folds):\n",
    "    cv = cross_val_score (rf_model, X_train, y_train, cv=n_folds, scoring='roc_auc', n_jobs=-1)\n",
    "    print ('Cross validation score:')\n",
    "    print ('\\t' + str(np.mean(cv)) + ' +/- ' + str(np.std(cv)))\n",
    "    print ('Raw: ' + str(cv))\n",
    "    \n",
    "    return cv\n",
    "        \n",
    "def sanity_check (y_train, submission):\n",
    "    print ()\n",
    "    print (\"Sanity Check:\")\n",
    "    print (\"Mean of Y in training data:\", y_train.mean())\n",
    "    print (\"Versus Mean of predicted Y:\", submission['Y'].mean())\n",
    "    print (\"Difference = \", round(abs(y_train.mean() - submission['Y'].mean()),3))\n",
    "    print (\"If these differ by more than 0.01 or so, something may have gone wrong.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 19s, sys: 632 ms, total: 2min 20s\n",
      "Wall time: 19 s\n",
      "CPU times: user 2min 44s, sys: 412 ms, total: 2min 45s\n",
      "Wall time: 23.4 s\n",
      "CPU times: user 2min 42s, sys: 288 ms, total: 2min 42s\n",
      "Wall time: 23 s\n",
      "CPU times: user 2min 36s, sys: 268 ms, total: 2min 37s\n",
      "Wall time: 21.6 s\n",
      "CPU times: user 3min 14s, sys: 408 ms, total: 3min 15s\n",
      "Wall time: 26.3 s\n",
      "\n",
      "\n",
      "Sanity Check:\n",
      "Mean of Y in training data: 0.0672226889075563\n",
      "Versus Mean of predicted Y: 0.0665233257651052\n",
      "Difference =  0.001\n",
      "If these differ by more than 0.01 or so, something may have gone wrong.\n",
      "\n",
      "Sanity Check:\n",
      "Mean of Y in training data: 0.0672226889075563\n",
      "Versus Mean of predicted Y: 0.0665348865517918\n",
      "Difference =  0.001\n",
      "If these differ by more than 0.01 or so, something may have gone wrong.\n"
     ]
    }
   ],
   "source": [
    "def rf_base_model():\n",
    "    rf_model = RandomForestClassifier(criterion='entropy', max_depth=10, n_estimators=1000, n_jobs=-1)\n",
    "    rf_model = train_and_save_preds(rf_model, __folds, \n",
    "                                    X_train_basic, y_train_basic, \n",
    "                                    test_basic, 'RandomForest')\n",
    "    return rf_model\n",
    "\n",
    "rf_model = rf_base_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation results:\n",
      "\t0.859302094507 +/- 0.00717215755717\n",
      "Raw: [ 0.84927165  0.86539933  0.8648292   0.86512308  0.85188722]\n"
     ]
    }
   ],
   "source": [
    "rf_cv = cross_val (rf_model, X_train_basic, y_train_basic, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0xa76bb46c>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhMAAAFyCAYAAABP41hGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xa76acfec>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.Series(index=X_train_basic.columns, data=rf_model.feature_importances_).sort_values().plot(kind='barh', title='Variable Importances')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def xgb_base_model(X, y, T, name):\n",
    "    xgb_model = XGBClassifier(max_depth=6, n_estimators=100, subsample=1,\n",
    "                              learning_rate=0.1, gamma=5, reg_lambda=10, \n",
    "                              objective='binary:logistic', silent=1, \n",
    "                              colsample_bytree=0.9, colsample_bylevel=0.25)\n",
    "    train_and_save_preds(xgb_model, __folds,\n",
    "                         X, y, T, name)\n",
    "    \n",
    "    return xgb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.64 s, sys: 12 ms, total: 2.65 s\n",
      "Wall time: 2.65 s\n",
      "CPU times: user 2.69 s, sys: 8 ms, total: 2.7 s\n",
      "Wall time: 2.7 s\n",
      "CPU times: user 2.68 s, sys: 4 ms, total: 2.69 s\n",
      "Wall time: 2.69 s\n",
      "CPU times: user 2.71 s, sys: 8 ms, total: 2.72 s\n",
      "Wall time: 2.71 s\n",
      "CPU times: user 2.75 s, sys: 0 ns, total: 2.75 s\n",
      "Wall time: 2.75 s\n",
      "\n",
      "\n",
      "Sanity Check:\n",
      "Mean of Y in training data: 0.0672226889075563\n",
      "Versus Mean of predicted Y: 0.06688789710228518\n",
      "Difference =  0.0\n",
      "If these differ by more than 0.01 or so, something may have gone wrong.\n",
      "\n",
      "Sanity Check:\n",
      "Mean of Y in training data: 0.0672226889075563\n",
      "Versus Mean of predicted Y: 0.06702832195645858\n",
      "Difference =  0.0\n",
      "If these differ by more than 0.01 or so, something may have gone wrong.\n"
     ]
    }
   ],
   "source": [
    "xgb_missing_model = xgb_base_model(X_train_missing, y_train_missing, test_missing, 'XGB_Missing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.39 s, sys: 4 ms, total: 3.4 s\n",
      "Wall time: 3.39 s\n",
      "CPU times: user 3.32 s, sys: 8 ms, total: 3.33 s\n",
      "Wall time: 3.33 s\n",
      "CPU times: user 3.3 s, sys: 4 ms, total: 3.3 s\n",
      "Wall time: 3.3 s\n",
      "CPU times: user 3.31 s, sys: 0 ns, total: 3.31 s\n",
      "Wall time: 3.31 s\n",
      "CPU times: user 3.33 s, sys: 4 ms, total: 3.34 s\n",
      "Wall time: 3.34 s\n",
      "\n",
      "\n",
      "Sanity Check:\n",
      "Mean of Y in training data: 0.0672226889075563\n",
      "Versus Mean of predicted Y: 0.06692124521213584\n",
      "Difference =  0.0\n",
      "If these differ by more than 0.01 or so, something may have gone wrong.\n",
      "\n",
      "Sanity Check:\n",
      "Mean of Y in training data: 0.0672226889075563\n",
      "Versus Mean of predicted Y: 0.06705295858588281\n",
      "Difference =  0.0\n",
      "If these differ by more than 0.01 or so, something may have gone wrong.\n"
     ]
    }
   ],
   "source": [
    "xgb_inter_model = xgb_base_model (X_train_inter, y_train_inter, test_inter, 'XGB_Inter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation score:\n",
      "\t0.858001353404 +/- 0.00669072841179\n",
      "Raw: [ 0.84949179  0.86295852  0.86266574  0.86467268  0.85021804]\n"
     ]
    }
   ],
   "source": [
    "xgb_cv = cross_val (xgb_inter_model, X_train_inter, y_train_inter, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagged XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 54.3 s, sys: 196 ms, total: 54.5 s\n",
      "Wall time: 54.4 s\n",
      "CPU times: user 58.4 s, sys: 104 ms, total: 58.5 s\n",
      "Wall time: 58.4 s\n",
      "CPU times: user 55.7 s, sys: 96 ms, total: 55.8 s\n",
      "Wall time: 55.8 s\n",
      "CPU times: user 54.1 s, sys: 156 ms, total: 54.3 s\n",
      "Wall time: 54.2 s\n",
      "CPU times: user 55.7 s, sys: 160 ms, total: 55.9 s\n",
      "Wall time: 55.9 s\n",
      "\n",
      "\n",
      "Sanity Check:\n",
      "Mean of Y in training data: 0.0672226889075563\n",
      "Versus Mean of predicted Y: 0.06697918578255072\n",
      "Difference =  0.0\n",
      "If these differ by more than 0.01 or so, something may have gone wrong.\n",
      "\n",
      "Sanity Check:\n",
      "Mean of Y in training data: 0.0672226889075563\n",
      "Versus Mean of predicted Y: 0.06695628958879857\n",
      "Difference =  0.0\n",
      "If these differ by more than 0.01 or so, something may have gone wrong.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=XGBClassifier(base_score=0.5, colsample_bylevel=0.25, colsample_bytree=0.9,\n",
       "       gamma=5, learning_rate=0.1, max_delta_step=0, max_depth=6,\n",
       "       min_child_weight=1, missing=None, n_estimators=100, nthread=-1,\n",
       "       objective='binary:logistic', reg_alpha=0, reg_lambda=10,\n",
       "       scale_pos_weight=1, seed=0, silent=1, subsample=1),\n",
       "         bootstrap=True, bootstrap_features=True, max_features=0.75,\n",
       "         max_samples=0.8, n_estimators=30, n_jobs=1, oob_score=False,\n",
       "         random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_model = XGBClassifier(max_depth=6, n_estimators=100, subsample=1,\n",
    "                          learning_rate=0.1, gamma=5, reg_lambda=10, \n",
    "                          objective='binary:logistic', silent=1, \n",
    "                          colsample_bytree=0.9, colsample_bylevel=0.25)\n",
    "\n",
    "#num_bags = 10\n",
    "#sample = 0.75\n",
    "#xgb_bag_train = np.zeros((X_train_missing.shape[0], num_bags))\n",
    "#xgb_bag_test  = np.zeros((test_missing.shape[0], num_bags))\n",
    "\n",
    "xgb_bag_model = BaggingClassifier(base_estimator=xgb_model, n_estimators=30,\n",
    "                            max_samples=0.8, max_features=0.75, bootstrap_features=True)\n",
    "\n",
    "train_and_save_preds(xgb_bag_model, __folds,\n",
    "                     X_train_basic, y_train_basic,\n",
    "                     test_basic, 'XGB_Bagged')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.26 s, sys: 0 ns, total: 8.26 s\n",
      "Wall time: 8.25 s\n",
      "CPU times: user 8.11 s, sys: 0 ns, total: 8.11 s\n",
      "Wall time: 8.11 s\n",
      "CPU times: user 8.05 s, sys: 0 ns, total: 8.05 s\n",
      "Wall time: 8.04 s\n",
      "CPU times: user 8.03 s, sys: 4 ms, total: 8.04 s\n",
      "Wall time: 8.03 s\n",
      "CPU times: user 8.25 s, sys: 0 ns, total: 8.25 s\n",
      "Wall time: 8.25 s\n",
      "\n",
      "\n",
      "Sanity Check:\n",
      "Mean of Y in training data: 0.0672226889075563\n",
      "Versus Mean of predicted Y: 0.467319393506567\n",
      "Difference =  0.4\n",
      "If these differ by more than 0.01 or so, something may have gone wrong.\n",
      "\n",
      "Sanity Check:\n",
      "Mean of Y in training data: 0.0672226889075563\n",
      "Versus Mean of predicted Y: 0.4672393239480132\n",
      "Difference =  0.4\n",
      "If these differ by more than 0.01 or so, something may have gone wrong.\n"
     ]
    }
   ],
   "source": [
    "def ada_base_model():\n",
    "    ada_model = AdaBoostClassifier(algorithm= 'SAMME.R', base_estimator=DecisionTreeClassifier(max_depth=3),\n",
    "                                   learning_rate=1, n_estimators=50)\n",
    "    train_and_save_preds(ada_model, __folds,\n",
    "                         X_train_basic, y_train_basic,\n",
    "                         test_basic, 'Ada')\n",
    "    \n",
    "    return ada_model\n",
    "\n",
    "ada_model = ada_base_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation results:\n",
      "\t0.859376655581 +/- 0.00719312269772\n",
      "Raw: [ 0.84945484  0.86468432  0.86495731  0.86598144  0.85180537]\n"
     ]
    }
   ],
   "source": [
    "ada_cv = cross_val (ada_model, X_train_basic, y_train_basic, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22 s, sys: 36 ms, total: 22.1 s\n",
      "Wall time: 3.14 s\n",
      "CPU times: user 22 s, sys: 40 ms, total: 22 s\n",
      "Wall time: 3.12 s\n",
      "CPU times: user 22 s, sys: 60 ms, total: 22 s\n",
      "Wall time: 3.13 s\n",
      "CPU times: user 22 s, sys: 48 ms, total: 22.1 s\n",
      "Wall time: 3.12 s\n",
      "CPU times: user 21.7 s, sys: 36 ms, total: 21.8 s\n",
      "Wall time: 3.12 s\n",
      "\n",
      "\n",
      "Sanity Check:\n",
      "Mean of Y in training data: 0.0672226889075563\n",
      "Versus Mean of predicted Y: 0.06683513967484495\n",
      "Difference =  0.0\n",
      "If these differ by more than 0.01 or so, something may have gone wrong.\n",
      "\n",
      "Sanity Check:\n",
      "Mean of Y in training data: 0.0672226889075563\n",
      "Versus Mean of predicted Y: 0.06719678950539128\n",
      "Difference =  0.0\n",
      "If these differ by more than 0.01 or so, something may have gone wrong.\n"
     ]
    }
   ],
   "source": [
    "def ext_base_model():\n",
    "    ext_model = ExtraTreesClassifier(criterion='entropy', max_depth=20, n_estimators=250, n_jobs=-1)\n",
    "    train_and_save_preds(ext_model, __folds,\n",
    "                         X_train_basic, y_train_basic, \n",
    "                         test_basic, 'ExtraTrees')\n",
    "    \n",
    "    return ext_model\n",
    "\n",
    "ext_model = ext_base_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation results:\n",
      "\t0.859734147225 +/- 0.00743131046942\n",
      "Raw: [ 0.84910981  0.86486251  0.86561688  0.86669143  0.85239011]\n"
     ]
    }
   ],
   "source": [
    "ext_cv = cross_val (ext_model, X_train_basic, y_train_basic, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 276 ms, sys: 0 ns, total: 276 ms\n",
      "Wall time: 278 ms\n",
      "CPU times: user 292 ms, sys: 0 ns, total: 292 ms\n",
      "Wall time: 290 ms\n",
      "CPU times: user 536 ms, sys: 0 ns, total: 536 ms\n",
      "Wall time: 538 ms\n",
      "CPU times: user 296 ms, sys: 0 ns, total: 296 ms\n",
      "Wall time: 293 ms\n",
      "CPU times: user 204 ms, sys: 4 ms, total: 208 ms\n",
      "Wall time: 208 ms\n",
      "\n",
      "\n",
      "Sanity Check:\n",
      "Mean of Y in training data: 0.0672226889075563\n",
      "Versus Mean of predicted Y: 0.059741999999999996\n",
      "Difference =  0.007\n",
      "If these differ by more than 0.01 or so, something may have gone wrong.\n",
      "\n",
      "Sanity Check:\n",
      "Mean of Y in training data: 0.0672226889075563\n",
      "Versus Mean of predicted Y: 0.06085243409736389\n",
      "Difference =  0.006\n",
      "If these differ by more than 0.01 or so, something may have gone wrong.\n",
      "\n",
      "CPU times: user 336 ms, sys: 0 ns, total: 336 ms\n",
      "Wall time: 332 ms\n",
      "CPU times: user 284 ms, sys: 0 ns, total: 284 ms\n",
      "Wall time: 287 ms\n",
      "CPU times: user 548 ms, sys: 0 ns, total: 548 ms\n",
      "Wall time: 545 ms\n",
      "CPU times: user 292 ms, sys: 0 ns, total: 292 ms\n",
      "Wall time: 290 ms\n",
      "CPU times: user 208 ms, sys: 0 ns, total: 208 ms\n",
      "Wall time: 207 ms\n",
      "\n",
      "\n",
      "Sanity Check:\n",
      "Mean of Y in training data: 0.0672226889075563\n",
      "Versus Mean of predicted Y: 0.061051\n",
      "Difference =  0.006\n",
      "If these differ by more than 0.01 or so, something may have gone wrong.\n",
      "\n",
      "Sanity Check:\n",
      "Mean of Y in training data: 0.0672226889075563\n",
      "Versus Mean of predicted Y: 0.06158246329853194\n",
      "Difference =  0.006\n",
      "If these differ by more than 0.01 or so, something may have gone wrong.\n",
      "\n",
      "CPU times: user 328 ms, sys: 0 ns, total: 328 ms\n",
      "Wall time: 328 ms\n",
      "CPU times: user 316 ms, sys: 0 ns, total: 316 ms\n",
      "Wall time: 313 ms\n",
      "CPU times: user 540 ms, sys: 0 ns, total: 540 ms\n",
      "Wall time: 541 ms\n",
      "CPU times: user 288 ms, sys: 0 ns, total: 288 ms\n",
      "Wall time: 287 ms\n",
      "CPU times: user 196 ms, sys: 0 ns, total: 196 ms\n",
      "Wall time: 195 ms\n",
      "\n",
      "\n",
      "Sanity Check:\n",
      "Mean of Y in training data: 0.0672226889075563\n",
      "Versus Mean of predicted Y: 0.06277350000000001\n",
      "Difference =  0.004\n",
      "If these differ by more than 0.01 or so, something may have gone wrong.\n",
      "\n",
      "Sanity Check:\n",
      "Mean of Y in training data: 0.0672226889075563\n",
      "Versus Mean of predicted Y: 0.06292751710068403\n",
      "Difference =  0.004\n",
      "If these differ by more than 0.01 or so, something may have gone wrong.\n",
      "\n",
      "CPU times: user 324 ms, sys: 4 ms, total: 328 ms\n",
      "Wall time: 329 ms\n",
      "CPU times: user 296 ms, sys: 0 ns, total: 296 ms\n",
      "Wall time: 295 ms\n",
      "CPU times: user 344 ms, sys: 0 ns, total: 344 ms\n",
      "Wall time: 344 ms\n",
      "CPU times: user 328 ms, sys: 0 ns, total: 328 ms\n",
      "Wall time: 326 ms\n",
      "CPU times: user 220 ms, sys: 4 ms, total: 224 ms\n",
      "Wall time: 223 ms\n",
      "\n",
      "\n",
      "Sanity Check:\n",
      "Mean of Y in training data: 0.0672226889075563\n",
      "Versus Mean of predicted Y: 0.06434525\n",
      "Difference =  0.003\n",
      "If these differ by more than 0.01 or so, something may have gone wrong.\n",
      "\n",
      "Sanity Check:\n",
      "Mean of Y in training data: 0.0672226889075563\n",
      "Versus Mean of predicted Y: 0.06450758030321213\n",
      "Difference =  0.003\n",
      "If these differ by more than 0.01 or so, something may have gone wrong.\n",
      "\n",
      "CPU times: user 332 ms, sys: 0 ns, total: 332 ms\n",
      "Wall time: 331 ms\n",
      "CPU times: user 296 ms, sys: 0 ns, total: 296 ms\n",
      "Wall time: 297 ms\n",
      "CPU times: user 324 ms, sys: 0 ns, total: 324 ms\n",
      "Wall time: 324 ms\n",
      "CPU times: user 324 ms, sys: 0 ns, total: 324 ms\n",
      "Wall time: 325 ms\n",
      "CPU times: user 196 ms, sys: 0 ns, total: 196 ms\n",
      "Wall time: 197 ms\n",
      "\n",
      "\n",
      "Sanity Check:\n",
      "Mean of Y in training data: 0.0672226889075563\n",
      "Versus Mean of predicted Y: 0.06549050000000001\n",
      "Difference =  0.002\n",
      "If these differ by more than 0.01 or so, something may have gone wrong.\n",
      "\n",
      "Sanity Check:\n",
      "Mean of Y in training data: 0.0672226889075563\n",
      "Versus Mean of predicted Y: 0.0655407466298652\n",
      "Difference =  0.002\n",
      "If these differ by more than 0.01 or so, something may have gone wrong.\n",
      "\n",
      "CPU times: user 324 ms, sys: 0 ns, total: 324 ms\n",
      "Wall time: 325 ms\n",
      "CPU times: user 288 ms, sys: 0 ns, total: 288 ms\n",
      "Wall time: 287 ms\n",
      "CPU times: user 292 ms, sys: 4 ms, total: 296 ms\n",
      "Wall time: 295 ms\n",
      "CPU times: user 272 ms, sys: 0 ns, total: 272 ms\n",
      "Wall time: 269 ms\n",
      "CPU times: user 196 ms, sys: 0 ns, total: 196 ms\n",
      "Wall time: 198 ms\n",
      "\n",
      "\n",
      "Sanity Check:\n",
      "Mean of Y in training data: 0.0672226889075563\n",
      "Versus Mean of predicted Y: 0.066769\n",
      "Difference =  0.0\n",
      "If these differ by more than 0.01 or so, something may have gone wrong.\n",
      "\n",
      "Sanity Check:\n",
      "Mean of Y in training data: 0.0672226889075563\n",
      "Versus Mean of predicted Y: 0.06623139925597024\n",
      "Difference =  0.001\n",
      "If these differ by more than 0.01 or so, something may have gone wrong.\n",
      "\n",
      "CPU times: user 548 ms, sys: 0 ns, total: 548 ms\n",
      "Wall time: 548 ms\n",
      "CPU times: user 564 ms, sys: 0 ns, total: 564 ms\n",
      "Wall time: 563 ms\n",
      "CPU times: user 540 ms, sys: 4 ms, total: 544 ms\n",
      "Wall time: 546 ms\n",
      "CPU times: user 848 ms, sys: 0 ns, total: 848 ms\n",
      "Wall time: 843 ms\n",
      "CPU times: user 340 ms, sys: 0 ns, total: 340 ms\n",
      "Wall time: 338 ms\n",
      "\n",
      "\n",
      "Sanity Check:\n",
      "Mean of Y in training data: 0.0672226889075563\n",
      "Versus Mean of predicted Y: 0.067156625\n",
      "Difference =  0.0\n",
      "If these differ by more than 0.01 or so, something may have gone wrong.\n",
      "\n",
      "Sanity Check:\n",
      "Mean of Y in training data: 0.0672226889075563\n",
      "Versus Mean of predicted Y: 0.06682439172566902\n",
      "Difference =  0.0\n",
      "If these differ by more than 0.01 or so, something may have gone wrong.\n",
      "\n",
      "CPU times: user 304 ms, sys: 0 ns, total: 304 ms\n",
      "Wall time: 305 ms\n",
      "CPU times: user 292 ms, sys: 4 ms, total: 296 ms\n",
      "Wall time: 295 ms\n",
      "CPU times: user 276 ms, sys: 4 ms, total: 280 ms\n",
      "Wall time: 277 ms\n",
      "CPU times: user 288 ms, sys: 0 ns, total: 288 ms\n",
      "Wall time: 291 ms\n",
      "CPU times: user 208 ms, sys: 0 ns, total: 208 ms\n",
      "Wall time: 206 ms\n",
      "\n",
      "\n",
      "Sanity Check:\n",
      "Mean of Y in training data: 0.0672226889075563\n",
      "Versus Mean of predicted Y: 0.06710940625\n",
      "Difference =  0.0\n",
      "If these differ by more than 0.01 or so, something may have gone wrong.\n",
      "\n",
      "Sanity Check:\n",
      "Mean of Y in training data: 0.0672226889075563\n",
      "Versus Mean of predicted Y: 0.06688439412576504\n",
      "Difference =  0.0\n",
      "If these differ by more than 0.01 or so, something may have gone wrong.\n",
      "\n",
      "CPU times: user 308 ms, sys: 0 ns, total: 308 ms\n",
      "Wall time: 305 ms\n",
      "CPU times: user 288 ms, sys: 0 ns, total: 288 ms\n",
      "Wall time: 289 ms\n",
      "CPU times: user 292 ms, sys: 0 ns, total: 292 ms\n",
      "Wall time: 292 ms\n",
      "CPU times: user 448 ms, sys: 0 ns, total: 448 ms\n",
      "Wall time: 449 ms\n",
      "CPU times: user 224 ms, sys: 0 ns, total: 224 ms\n",
      "Wall time: 224 ms\n",
      "\n",
      "\n",
      "Sanity Check:\n",
      "Mean of Y in training data: 0.0672226889075563\n",
      "Versus Mean of predicted Y: 0.0663541796875\n",
      "Difference =  0.001\n",
      "If these differ by more than 0.01 or so, something may have gone wrong.\n",
      "\n",
      "Sanity Check:\n",
      "Mean of Y in training data: 0.0672226889075563\n",
      "Versus Mean of predicted Y: 0.06622190668876755\n",
      "Difference =  0.001\n",
      "If these differ by more than 0.01 or so, something may have gone wrong.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def knn_base_models():\n",
    "    knn_models = []\n",
    "    for i in range(1,10):\n",
    "        n = 2**i\n",
    "        knn_models.append(KNeighborsClassifier (n_neighbors=n, n_jobs=-1))\n",
    "        train_and_save_preds(knn_models[i-1], __folds,\n",
    "                             X_train_basic, y_train_basic, \n",
    "                             test_basic, 'KNN_'+str(n))\n",
    "        print('')\n",
    "        \n",
    "    return knn_models\n",
    "\n",
    "knn_models = knn_base_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation results:\n",
      "\t0.859734147225 +/- 0.00743131046942\n",
      "Raw: [ 0.84910981  0.86486251  0.86561688  0.86669143  0.85239011]\n",
      "Cross validation results:\n",
      "\t0.859734147225 +/- 0.00743131046942\n",
      "Raw: [ 0.84910981  0.86486251  0.86561688  0.86669143  0.85239011]\n",
      "Cross validation results:\n",
      "\t0.859734147225 +/- 0.00743131046942\n",
      "Raw: [ 0.84910981  0.86486251  0.86561688  0.86669143  0.85239011]\n",
      "Cross validation results:\n",
      "\t0.859734147225 +/- 0.00743131046942\n",
      "Raw: [ 0.84910981  0.86486251  0.86561688  0.86669143  0.85239011]\n",
      "Cross validation results:\n",
      "\t0.859734147225 +/- 0.00743131046942\n",
      "Raw: [ 0.84910981  0.86486251  0.86561688  0.86669143  0.85239011]\n",
      "Cross validation results:\n",
      "\t0.859734147225 +/- 0.00743131046942\n",
      "Raw: [ 0.84910981  0.86486251  0.86561688  0.86669143  0.85239011]\n",
      "Cross validation results:\n",
      "\t0.859734147225 +/- 0.00743131046942\n",
      "Raw: [ 0.84910981  0.86486251  0.86561688  0.86669143  0.85239011]\n",
      "Cross validation results:\n",
      "\t0.859734147225 +/- 0.00743131046942\n",
      "Raw: [ 0.84910981  0.86486251  0.86561688  0.86669143  0.85239011]\n",
      "Cross validation results:\n",
      "\t0.859734147225 +/- 0.00743131046942\n",
      "Raw: [ 0.84910981  0.86486251  0.86561688  0.86669143  0.85239011]\n"
     ]
    }
   ],
   "source": [
    "knn_cvs = []\n",
    "for knn_model in knn_models:\n",
    "    knn_cvs.append(cross_val (knn_model, X_train_basic, y_train_basic, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kian/anaconda2/envs/py35/lib/python3.5/site-packages/sklearn/linear_model/sag.py:286: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/kian/anaconda2/envs/py35/lib/python3.5/site-packages/sklearn/linear_model/sag.py:286: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/kian/anaconda2/envs/py35/lib/python3.5/site-packages/sklearn/linear_model/sag.py:286: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/kian/anaconda2/envs/py35/lib/python3.5/site-packages/sklearn/linear_model/sag.py:286: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/kian/anaconda2/envs/py35/lib/python3.5/site-packages/sklearn/linear_model/sag.py:286: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/kian/anaconda2/envs/py35/lib/python3.5/site-packages/sklearn/linear_model/sag.py:286: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/kian/anaconda2/envs/py35/lib/python3.5/site-packages/sklearn/linear_model/sag.py:286: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/kian/anaconda2/envs/py35/lib/python3.5/site-packages/sklearn/linear_model/sag.py:286: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/kian/anaconda2/envs/py35/lib/python3.5/site-packages/sklearn/linear_model/sag.py:286: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/kian/anaconda2/envs/py35/lib/python3.5/site-packages/sklearn/linear_model/sag.py:286: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/kian/anaconda2/envs/py35/lib/python3.5/site-packages/sklearn/linear_model/sag.py:286: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/kian/anaconda2/envs/py35/lib/python3.5/site-packages/sklearn/linear_model/sag.py:286: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/kian/anaconda2/envs/py35/lib/python3.5/site-packages/sklearn/linear_model/sag.py:286: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/kian/anaconda2/envs/py35/lib/python3.5/site-packages/sklearn/linear_model/sag.py:286: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/kian/anaconda2/envs/py35/lib/python3.5/site-packages/sklearn/linear_model/sag.py:286: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/kian/anaconda2/envs/py35/lib/python3.5/site-packages/sklearn/linear_model/sag.py:286: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/kian/anaconda2/envs/py35/lib/python3.5/site-packages/sklearn/linear_model/sag.py:286: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/kian/anaconda2/envs/py35/lib/python3.5/site-packages/sklearn/linear_model/sag.py:286: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/kian/anaconda2/envs/py35/lib/python3.5/site-packages/sklearn/linear_model/sag.py:286: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/kian/anaconda2/envs/py35/lib/python3.5/site-packages/sklearn/linear_model/sag.py:286: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/kian/anaconda2/envs/py35/lib/python3.5/site-packages/sklearn/linear_model/sag.py:286: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/kian/anaconda2/envs/py35/lib/python3.5/site-packages/sklearn/linear_model/sag.py:286: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/kian/anaconda2/envs/py35/lib/python3.5/site-packages/sklearn/linear_model/sag.py:286: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/kian/anaconda2/envs/py35/lib/python3.5/site-packages/sklearn/linear_model/sag.py:286: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/kian/anaconda2/envs/py35/lib/python3.5/site-packages/sklearn/linear_model/sag.py:286: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/kian/anaconda2/envs/py35/lib/python3.5/site-packages/sklearn/linear_model/sag.py:286: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/kian/anaconda2/envs/py35/lib/python3.5/site-packages/sklearn/linear_model/sag.py:286: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/kian/anaconda2/envs/py35/lib/python3.5/site-packages/sklearn/linear_model/sag.py:286: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/kian/anaconda2/envs/py35/lib/python3.5/site-packages/sklearn/linear_model/sag.py:286: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/kian/anaconda2/envs/py35/lib/python3.5/site-packages/sklearn/linear_model/sag.py:286: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/kian/anaconda2/envs/py35/lib/python3.5/site-packages/sklearn/linear_model/sag.py:286: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/kian/anaconda2/envs/py35/lib/python3.5/site-packages/sklearn/linear_model/sag.py:286: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/kian/anaconda2/envs/py35/lib/python3.5/site-packages/sklearn/linear_model/sag.py:286: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.66 s, sys: 688 ms, total: 10.4 s\n",
      "Wall time: 1min 25s\n",
      "0.836403471587\n",
      "{'solver': 'liblinear', 'C': 0.01, 'tol': 0.01, 'max_iter': 100}\n"
     ]
    }
   ],
   "source": [
    "def gridsearch_logreg ():\n",
    "    model = LogisticRegression()\n",
    "    model_grid = {\n",
    "        'C': [0.001, 0.01, 0.1, 0.5, 1.0],\n",
    "        'max_iter': [100, 200],\n",
    "        'tol': [1e-4, 1e-3, 1e-2],\n",
    "        'solver': ['liblinear', 'sag']\n",
    "    }\n",
    "\n",
    "    grid = GridSearchCV(model, model_grid, cv=5, scoring='roc_auc', n_jobs=8)\n",
    "    %time grid.fit(X_train_log, y_train_log) \n",
    "\n",
    "    return grid\n",
    "\n",
    "grid = gridsearch_logreg()\n",
    "print (grid.best_score_)\n",
    "print (grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 188 ms, sys: 4 ms, total: 192 ms\n",
      "Wall time: 194 ms\n",
      "CPU times: user 584 ms, sys: 16 ms, total: 600 ms\n",
      "Wall time: 150 ms\n",
      "CPU times: user 648 ms, sys: 12 ms, total: 660 ms\n",
      "Wall time: 164 ms\n",
      "CPU times: user 596 ms, sys: 16 ms, total: 612 ms\n",
      "Wall time: 154 ms\n",
      "CPU times: user 592 ms, sys: 16 ms, total: 608 ms\n",
      "Wall time: 151 ms\n",
      "\n",
      "\n",
      "Sanity Check:\n",
      "Mean of Y in training data: 0.0672226889075563\n",
      "Versus Mean of predicted Y: 0.06710676938600431\n",
      "Difference =  0.0\n",
      "If these differ by more than 0.01 or so, something may have gone wrong.\n",
      "\n",
      "Sanity Check:\n",
      "Mean of Y in training data: 0.0672226889075563\n",
      "Versus Mean of predicted Y: 0.06722070173036462\n",
      "Difference =  0.0\n",
      "If these differ by more than 0.01 or so, something may have gone wrong.\n"
     ]
    }
   ],
   "source": [
    "def logreg_base_model():\n",
    "    logreg_model = LogisticRegression(C=0.01, tol=0.01, max_iter=100)\n",
    "    train_and_save_preds(logreg_model, __folds,\n",
    "                         X_train_log, y_train_log, \n",
    "                         test_log, 'LogReg')\n",
    "    \n",
    "    return logreg_model\n",
    "\n",
    "logreg_model = logreg_base_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation score:\n",
      "\t0.859672005955 +/- 0.00709809857421\n",
      "Raw: [ 0.84976323  0.86470426  0.86521929  0.86631554  0.85235772]\n"
     ]
    }
   ],
   "source": [
    "logreg_cv = cross_val (logreg_model, X_train_log, y_train_log, 5)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
